<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>FarmAssist â€” AI Farm Assistant (Multilingual)</title>

  <!-- Tailwind CDN -->
  <script src="https://cdn.tailwindcss.com"></script>

  <!-- Lucide icons -->
  <link href="https://unpkg.com/lucide-static/font/lucide.css" rel="stylesheet">

  <!-- Google fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">

  <style>
    :root{
      --farm-green:#22c55e;
      --farm-green-dark:#15803d;
      --farm-black:#000000;
      --glass: rgba(34,197,94,0.08);
    }

    html,body { height:100%; }
    body{
      margin:0;
      font-family: "Inter", system-ui, -apple-system, "Segoe UI", Roboto, "Helvetica Neue", Arial;
      background: radial-gradient(1200px 600px at 10% 10%, rgba(16,16,16,0.6), #000 40%), #000;
      color: #d1fae5;
      -webkit-font-smoothing:antialiased;
      -moz-osx-font-smoothing:grayscale;
    }

    /* glass panel look */
    .glass {
      background: var(--glass);
      backdrop-filter: blur(8px);
      border: 1px solid rgba(34,197,94,0.12);
    }

    /* waveform bars base */
    .wave {
      width: 10px;
      height: 14px;
      margin: 0 5px;
      background: rgba(34,197,94,0.95);
      border-radius: 5px;
      transition: height 120ms linear, background 200ms linear;
      transform-origin: bottom;
    }

    .wave.animated {
      animation: wavePulse 1.2s infinite ease-in-out;
    }
    @keyframes wavePulse {
      0%,100% { transform: scaleY(0.5); }
      50% { transform: scaleY(1.15); }
    }

    @keyframes fadeUp {
      from { opacity: 0; transform: translateY(6px); }
      to { opacity: 1; transform: translateY(0); }
    }
    .fade-up { animation: fadeUp 260ms ease both; }

    .mic-active {
      box-shadow: 0 12px 40px rgba(34,197,94,0.12), 0 0 40px rgba(34,197,94,0.15);
    }
    .mic-listening {
      box-shadow: 0 18px 60px rgba(239,68,68,0.18), 0 0 50px rgba(239,68,68,0.12);
    }

    .history-scroll::-webkit-scrollbar { width:8px; height:8px; }
    .history-scroll::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.06); border-radius: 8px; }

    .permission-denied {
      background: rgba(239,68,68,0.1);
      border-color: rgba(239,68,68,0.3);
      color: #fca5a5;
    }

    .response-bubble {
      background: linear-gradient(135deg, rgba(34,197,94,0.1), rgba(34,197,94,0.05));
      border-left: 3px solid rgba(34,197,94,0.5);
    }

    .user-bubble {
      background: linear-gradient(135deg, rgba(59,130,246,0.1), rgba(59,130,246,0.05));
      border-left: 3px solid rgba(59,130,246,0.5);
    }

    .language-btn.active {
      background: rgba(34,197,94,0.3);
      color: #22c55e;
    }

    .speaking-indicator {
      animation: pulse 2s infinite;
    }

    @keyframes pulse {
      0%, 100% { opacity: 1; }
      50% { opacity: 0.5; }
    }

    @media (min-width:768px) {
      .title-small{ font-size:20px; }
      .transcript{ font-size:28px; }
    }
    @media (max-width:420px) {
      .wave { margin: 0 3px; width: 8px; }
      .mic-btn { width:72px; height:72px; }
    }
  </style>
</head>
<body class="min-h-screen flex flex-col">

  <!-- Top header -->
  <header class="w-full max-w-3xl mx-auto px-5 py-4 flex items-center justify-between glass rounded-b-xl">
    <div class="flex items-center gap-3">
      <button id="btn-back" class="p-2 rounded-full hover:bg-white/3 transition" aria-label="Back">
        <i class="lucide lucide-chevron-left"></i>
      </button>
      <div class="flex items-center gap-2">
        <i class="lucide lucide-sprout text-2xl text-emerald-300"></i>
        <div>
          <div class="font-semibold text-white">FarmAssist</div>
          <div class="text-xs text-emerald-200/70">AI Farm Assistant</div>
        </div>
      </div>
    </div>

    <div class="flex items-center gap-3">
      <!-- Language Selector -->
      <div class="flex items-center gap-1">
        <button class="language-btn px-2 py-1 text-xs rounded glass hover:bg-emerald-500/20 transition active" data-lang="en" data-code="en-US">EN</button>
        <button class="language-btn px-2 py-1 text-xs rounded glass hover:bg-emerald-500/20 transition" data-lang="hi" data-code="hi-IN">à¤¹à¤¿</button>
        <button class="language-btn px-2 py-1 text-xs rounded glass hover:bg-emerald-500/20 transition" data-lang="ta" data-code="ta-IN">à®¤</button>
        <button class="language-btn px-2 py-1 text-xs rounded glass hover:bg-emerald-500/20 transition" data-lang="ml" data-code="ml-IN">à´®</button>
      </div>
      
      <div id="onlineIndicator" class="flex items-center gap-2 px-3 py-1 rounded-full glass text-sm">
        <i id="wifiIcon" class="lucide lucide-wifi"></i>
        <span id="onlineText" class="text-xs">Online</span>
      </div>
      <button id="btn-settings" class="p-2 rounded-full hover:bg-white/3 transition" aria-label="Settings">
        <i class="lucide lucide-settings"></i>
      </button>
    </div>
  </header>

  <!-- Main content area -->
  <main class="flex-1 w-full max-w-3xl mx-auto px-6 py-8 flex flex-col items-center justify-center">

    <!-- live status -->
    <div id="statusPill" class="glass px-4 py-2 rounded-full inline-flex items-center gap-3 mb-6 select-none cursor-default">
      <i id="statusPillIcon" class="lucide lucide-mic-off"></i>
      <span id="statusText" class="text-sm">Checking microphone permissions...</span>
    </div>

    <!-- Permission alert -->
    <div id="permissionAlert" class="w-full max-w-2xl mb-6 p-4 rounded-lg permission-denied hidden">
      <div class="flex items-start gap-3">
        <i class="lucide lucide-alert-triangle text-red-400 mt-0.5"></i>
        <div class="flex-1">
          <h4 class="font-medium text-red-300 mb-1">Microphone Permission Required</h4>
          <p class="text-sm text-red-200/80 mb-3">FarmAssist needs microphone access to listen to your voice commands.</p>
          <button id="retryPermissions" class="px-3 py-1 bg-red-500/20 text-red-300 rounded text-sm hover:bg-red-500/30 transition">
            Retry Permission Request
          </button>
        </div>
      </div>
    </div>

    <!-- live transcript -->
    <div id="transcriptionContainer" class="w-full max-w-2xl text-center mb-8">
      <p id="transcript" class="transcript text-2xl md:text-3xl text-emerald-100 font-light min-h-[56px]"></p>
    </div>

    <!-- centered waveform -->
    <div id="visualizer" class="w-full flex justify-center items-end mb-8" aria-hidden="true">
      <div id="waveRow" class="flex items-end">
        <div class="wave" data-index="0"></div>
        <div class="wave" data-index="1"></div>
        <div class="wave" data-index="2"></div>
        <div class="wave" data-index="3"></div>
        <div class="wave" data-index="4"></div>
        <div class="wave" data-index="5"></div>
        <div class="wave" data-index="6"></div>
        <div class="wave" data-index="7"></div>
        <div class="wave" data-index="8"></div>
        <div class="wave" data-index="9"></div>
        <div class="wave" data-index="10"></div>
      </div>
    </div>

    <!-- conversation history -->
    <div class="w-full max-w-2xl mb-6">
      <div class="flex justify-between items-center mb-3">
        <h3 class="text-sm font-medium text-emerald-200">Conversation History</h3>
        <div class="flex items-center gap-2">
          <button id="btn-clear" class="text-xs px-3 py-1 rounded-full glass hover:bg-emerald-800/30 transition">Clear</button>
          <button id="btn-toggle-history" class="text-xs px-3 py-1 rounded-full glass hover:bg-emerald-800/30 transition">Expand</button>
        </div>
      </div>

      <div id="historyPanel" class="glass rounded-xl p-3 max-h-48 overflow-auto history-scroll">
        <div id="historyList" class="space-y-3">
          <!-- History items inserted here -->
        </div>
        <div id="historyEmpty" class="text-xs text-emerald-200/40 text-center py-4">No conversation history yet â€” your voice interactions will appear here.</div>
      </div>
    </div>

  </main>

  <!-- bottom controls -->
  <footer class="w-full max-w-3xl mx-auto px-6 pb-8">
    <!-- Voice controls -->
    <div class="flex justify-center items-center gap-4 mb-4">
      <button id="stopSpeakingBtn" class="px-4 py-2 bg-red-500/20 text-red-300 rounded-lg hover:bg-red-500/30 transition hidden">
        <i class="lucide lucide-square mr-2"></i>Stop Speaking
      </button>
    </div>
    
    <!-- Mic button -->
    <div class="flex justify-center">
      <button id="micBtn" class="mic-btn mic-idle w-24 h-24 md:w-28 md:h-28 rounded-full bg-emerald-500 flex items-center justify-center text-black font-semibold mic-active transition-transform hover:scale-105 focus:outline-none disabled:opacity-50 disabled:cursor-not-allowed" aria-label="Start listening" title="Tap to start/stop">
        <i id="micBtnIcon" class="lucide lucide-mic text-3xl md:text-4xl"></i>
      </button>
    </div>
  </footer>

  <script>
    // ==============================================
    // CONFIGURATION - PASTE YOUR GEMINI API KEY HERE
    // ==============================================
    const GEMINI_API_KEY = 'AIzaSyAZAzGRYMOhiHVjbuI59ohafnDVJ5AYu0M'; // <- Paste your Gemini API key here between the quotes

    // Configuration & DOM nodes
    const STATUS_PILL = document.getElementById('statusPill');
    const STATUS_TEXT = document.getElementById('statusText');
    const STATUS_ICON = document.getElementById('statusPillIcon');
    const TRANSCRIPT = document.getElementById('transcript');
    const MIC_BTN = document.getElementById('micBtn');
    const MIC_ICON = document.getElementById('micBtnIcon');
    const WAVE_ELEMENTS = Array.from(document.querySelectorAll('.wave'));
    const HISTORY_LIST = document.getElementById('historyList');
    const HISTORY_PANEL = document.getElementById('historyPanel');
    const HISTORY_EMPTY = document.getElementById('historyEmpty');
    const BTN_CLEAR = document.getElementById('btn-clear');
    const BTN_TOGGLE_HISTORY = document.getElementById('btn-toggle-history');
    const ONLINE_TEXT = document.getElementById('onlineText');
    const WIFI_ICON = document.getElementById('wifiIcon');
    const PERMISSION_ALERT = document.getElementById('permissionAlert');
    const RETRY_PERMISSIONS = document.getElementById('retryPermissions');
    const STOP_SPEAKING_BTN = document.getElementById('stopSpeakingBtn');
    const LANGUAGE_BTNS = document.querySelectorAll('.language-btn');

    const LOCAL_KEY = 'farmassist-conversation-history-v2';
    const LANGUAGE_STORAGE = 'farmassist-language';
    const MAX_HISTORY = 50;

    // Language configurations
    const LANGUAGES = {
      'en': { name: 'English', code: 'en-US', voice: 'en-US' },
      'hi': { name: 'à¤¹à¤¿à¤‚à¤¦à¥€', code: 'hi-IN', voice: 'hi-IN' },
      'ta': { name: 'à®¤à®®à®¿à®´à¯', code: 'ta-IN', voice: 'ta-IN' },
      'ml': { name: 'à´®à´²à´¯à´¾à´³à´‚', code: 'ml-IN', voice: 'ml-IN' }
    };

    // State
    let isListening = false;
    let isSpeaking = false;
    let recognition = null;
    let speechSynthesis = window.speechSynthesis;
    let currentUtterance = null;
    let audioStream = null;
    let audioContext = null;
    let analyser = null;
    let rafId = null;
    let interimTranscript = '';
    let micPermissionGranted = false;
    let speechRecognitionSupported = false;
    let currentLanguage = 'en';
    let availableVoices = [];
    let history = [];

    // Load configuration
    function loadConfig() {
      currentLanguage = localStorage.getItem(LANGUAGE_STORAGE) || 'en';
      
      // Update language UI
      LANGUAGE_BTNS.forEach(btn => {
        btn.classList.remove('active');
        if (btn.dataset.lang === currentLanguage) {
          btn.classList.add('active');
        }
      });
    }

    // Auto-detect language from text
    function detectLanguage(text) {
      const devanagariRegex = /[\u0900-\u097F]/;
      const tamilRegex = /[\u0B80-\u0BFF]/;
      const malayalamRegex = /[\u0D00-\u0D7F]/;
      
      if (devanagariRegex.test(text)) return 'hi';
      if (tamilRegex.test(text)) return 'ta';
      if (malayalamRegex.test(text)) return 'ml';
      return 'en';
    }

    // Language switching
    LANGUAGE_BTNS.forEach(btn => {
      btn.addEventListener('click', () => {
        const newLang = btn.dataset.lang;
        if (newLang !== currentLanguage) {
          currentLanguage = newLang;
          localStorage.setItem(LANGUAGE_STORAGE, currentLanguage);
          
          LANGUAGE_BTNS.forEach(b => b.classList.remove('active'));
          btn.classList.add('active');
          
          // Update speech recognition language
          if (recognition) {
            recognition.lang = LANGUAGES[currentLanguage].code;
          }
          
          STATUS_TEXT.textContent = `Language switched to ${LANGUAGES[currentLanguage].name}`;
          setTimeout(() => {
            STATUS_TEXT.textContent = 'Ready to assist you';
          }, 2000);
        }
      });
    });

    // History management
    function loadHistory() {
      const raw = localStorage.getItem(LOCAL_KEY);
      if (!raw) return;
      try {
        const parsed = JSON.parse(raw);
        history = parsed.map(e => ({ ...e, timestamp: new Date(e.timestamp) }));
      } catch (e) {
        console.error('Failed to parse history', e);
        history = [];
      }
    }

    function saveHistory() {
      localStorage.setItem(LOCAL_KEY, JSON.stringify(history.slice(0, MAX_HISTORY)));
    }

    function renderHistory() {
      HISTORY_LIST.innerHTML = '';
      if (history.length === 0) {
        HISTORY_EMPTY.style.display = 'block';
        return;
      }
      HISTORY_EMPTY.style.display = 'none';
      
      history.forEach(entry => {
        const el = document.createElement('div');
        const isResponse = entry.type === 'response';
        el.className = `fade-up p-3 rounded-lg ${isResponse ? 'response-bubble' : 'user-bubble'}`;
        
        el.innerHTML = `
          <div class="flex items-start justify-between gap-3">
            <div class="flex-1">
              <div class="text-xs opacity-70 mb-1">
                ${isResponse ? 'ðŸ¤– FarmAssist' : 'ðŸ‘¤ You'} â€¢ ${timeAgo(entry.timestamp)}
                ${entry.language ? `â€¢ ${LANGUAGES[entry.language]?.name || entry.language}` : ''}
              </div>
              <div class="text-sm">${escapeHtml(entry.content)}</div>
            </div>
            <div class="flex gap-2">
              ${isResponse ? `<button class="speak-btn text-xs px-2 py-1 rounded hover:bg-white/10" data-text="${escapeHtml(entry.content)}" data-lang="${entry.language || currentLanguage}">ðŸ”Š</button>` : ''}
              <button data-id="${entry.id}" class="remove-entry text-xs px-2 py-1 rounded hover:bg-white/10">Ã—</button>
            </div>
          </div>
        `;
        HISTORY_LIST.appendChild(el);
      });

      // Wire up buttons
      HISTORY_LIST.querySelectorAll('.remove-entry').forEach(btn => {
        btn.addEventListener('click', () => {
          const id = btn.getAttribute('data-id');
          history = history.filter(h => h.id !== id);
          saveHistory();
          renderHistory();
        });
      });

      HISTORY_LIST.querySelectorAll('.speak-btn').forEach(btn => {
        btn.addEventListener('click', () => {
          const text = btn.getAttribute('data-text');
          const lang = btn.getAttribute('data-lang');
          speakText(text, lang);
        });
      });
    }

    function addHistoryEntry(content, type = 'user', language = null) {
      const trimmed = (content || '').trim();
      if (!trimmed) return;
      
      const entry = {
        id: Date.now().toString(36) + Math.random().toString(36).slice(2, 6),
        content: trimmed,
        timestamp: new Date(),
        type,
        language: language || currentLanguage
      };
      
      history.unshift(entry);
      history = history.slice(0, MAX_HISTORY);
      saveHistory();
      renderHistory();
      
      // Auto-scroll to show latest
      setTimeout(() => {
        HISTORY_LIST.scrollTop = 0;
      }, 100);
    }

    // Text-to-Speech using Web Speech API (free)
    function initVoices() {
      availableVoices = speechSynthesis.getVoices();
      console.log('Available voices:', availableVoices.length);
    }

    function findBestVoice(language) {
      const langCode = LANGUAGES[language]?.voice || 'en-US';
      
      // Try to find exact match
      let voice = availableVoices.find(v => v.lang === langCode);
      
      // Fallback to language family
      if (!voice) {
        const langPrefix = langCode.split('-')[0];
        voice = availableVoices.find(v => v.lang.startsWith(langPrefix));
      }
      
      // Fallback to any voice
      if (!voice && availableVoices.length > 0) {
        voice = availableVoices[0];
      }
      
      console.log(`Voice for ${language} (${langCode}):`, voice?.name || 'Not found');
      return voice;
    }

    function speakText(text, language = currentLanguage) {
      if (!text.trim()) return;
      
      // Stop current speech
      if (isSpeaking) {
        speechSynthesis.cancel();
      }
      
      const utterance = new SpeechSynthesisUtterance(text);
      const voice = findBestVoice(language);
      
      if (voice) {
        utterance.voice = voice;
      }
      
      utterance.lang = LANGUAGES[language]?.voice || 'en-US';
      utterance.rate = 0.9;
      utterance.pitch = 1;
      utterance.volume = 1;
      
      utterance.onstart = () => {
        isSpeaking = true;
        currentUtterance = utterance;
        STOP_SPEAKING_BTN.classList.remove('hidden');
        STATUS_ICON.classList.add('speaking-indicator');
        STATUS_TEXT.textContent = 'Speaking...';
      };
      
      utterance.onend = () => {
        isSpeaking = false;
        currentUtterance = null;
        STOP_SPEAKING_BTN.classList.add('hidden');
        STATUS_ICON.classList.remove('speaking-indicator');
        STATUS_TEXT.textContent = 'Ready to assist you';
      };
      
      utterance.onerror = (event) => {
        console.error('Speech synthesis error:', event);
        isSpeaking = false;
        currentUtterance = null;
        STOP_SPEAKING_BTN.classList.add('hidden');
        STATUS_ICON.classList.remove('speaking-indicator');
        STATUS_TEXT.textContent = 'Speech error occurred';
      };
      
      speechSynthesis.speak(utterance);
    }

    STOP_SPEAKING_BTN.addEventListener('click', () => {
      if (isSpeaking) {
        speechSynthesis.cancel();
      }
    });

    // Initialize voices when they're loaded
    speechSynthesis.addEventListener('voiceschanged', initVoices);
    initVoices(); // Call immediately in case voices are already loaded

    // Gemini API Integration
    async function callGeminiAPI(userMessage) {
      if (!GEMINI_API_KEY) {
        return "Please configure your Gemini API key in the code (GEMINI_API_KEY variable) to get AI responses.";
      }

      try {
        const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=${GEMINI_API_KEY}`, {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
          },
          body: JSON.stringify({
            contents: [{
              parts: [{
                text: `You are FarmAssist, an AI assistant specialized in farming and agriculture. You help farmers with crop management, pest control, soil health, weather planning, and general farming advice. 

User language context: The user is speaking in ${LANGUAGES[currentLanguage].name}. Please respond in the same language they used.

User message: "${userMessage}"

Please provide helpful, practical farming advice. Keep responses conversational and not too long.`
              }]
            }],
            generationConfig: {
              temperature: 0.7,
              topK: 40,
              topP: 0.95,
              maxOutputTokens: 512,
            }
          })
        });

        if (!response.ok) {
          throw new Error(`API request failed: ${response.status}`);
        }

        const data = await response.json();
        
        if (data.candidates && data.candidates[0] && data.candidates[0].content) {
          return data.candidates[0].content.parts[0].text;
        } else {
          throw new Error('Unexpected API response format');
        }
      } catch (error) {
        console.error('Gemini API Error:', error);
        return `Sorry, I'm having trouble connecting to my knowledge base right now. Error: ${error.message}`;
      }
    }

    // Process user input and get AI response
    async function processUserInput(userText) {
      if (!userText.trim()) return;

      // Detect and switch language if needed
      const detectedLang = detectLanguage(userText);
      if (detectedLang !== currentLanguage) {
        currentLanguage = detectedLang;
        localStorage.setItem(LANGUAGE_STORAGE, currentLanguage);
        
        // Update UI
        LANGUAGE_BTNS.forEach(btn => {
          btn.classList.remove('active');
          if (btn.dataset.lang === currentLanguage) {
            btn.classList.add('active');
          }
        });
        
        // Update recognition language
        if (recognition) {
          recognition.lang = LANGUAGES[currentLanguage].code;
        }
      }

      // Add user message to history
      addHistoryEntry(userText, 'user', currentLanguage);
      
      // Show thinking status
      STATUS_TEXT.textContent = 'Thinking...';
      STATUS_ICON.className = 'lucide lucide-brain';
      
      try {
        // Get AI response
        const aiResponse = await callGeminiAPI(userText);
        
        // Add AI response to history
        addHistoryEntry(aiResponse, 'response', currentLanguage);
        
        // Speak the response
        speakText(aiResponse, currentLanguage);
        
      } catch (error) {
        console.error('Error processing user input:', error);
        STATUS_TEXT.textContent = 'Error occurred';
        setTimeout(() => {
          STATUS_TEXT.textContent = 'Ready to assist you';
        }, 3000);
      }
    }

    // Utility functions
    function timeAgo(date) {
      const now = new Date();
      const diff = Math.floor((now - (new Date(date))) / 1000);
      if (diff < 60) return `${diff}s ago`;
      if (diff < 3600) return `${Math.floor(diff / 60)}m ago`;
      if (diff < 86400) return `${Math.floor(diff / 3600)}h ago`;
      return (new Date(date)).toLocaleString();
    }

    function escapeHtml(text) {
      return text.replace(/[&<>"']/g, (s) => ({ '&': '&amp;', '<': '&lt;', '>': '&gt;', '"': '&quot;', "'": '&#39;' }[s]));
    }

    // History controls
    BTN_CLEAR.addEventListener('click', () => {
      if (!confirm('Clear all conversation history?')) return;
      history = [];
      saveHistory();
      renderHistory();
    });

    BTN_TOGGLE_HISTORY.addEventListener('click', () => {
      if (HISTORY_PANEL.style.maxHeight && HISTORY_PANEL.style.maxHeight !== '0px') {
        HISTORY_PANEL.style.maxHeight = '0';
        BTN_TOGGLE_HISTORY.textContent = 'Expand';
      } else {
        HISTORY_PANEL.style.maxHeight = '12rem';
        BTN_TOGGLE_HISTORY.textContent = 'Collapse';
      }
    });

    // Microphone permission handling
    async function checkMicrophonePermission() {
      try {
        if (navigator.permissions) {
          const permissionResult = await navigator.permissions.query({ name: 'microphone' });
          console.log('Initial mic permission state:', permissionResult.state);
          
          if (permissionResult.state === 'granted') {
            micPermissionGranted = true;
            PERMISSION_ALERT.classList.add('hidden');
            return true;
          } else if (permissionResult.state === 'denied') {
            showPermissionDeniedUI();
            return false;
          }
        }

        const stream = await navigator.mediaDevices.getUserMedia({ 
          audio: { 
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true
          } 
        });
        
        stream.getTracks().forEach(track => track.stop());
        micPermissionGranted = true;
        PERMISSION_ALERT.classList.add('hidden');
        console.log('Microphone permission granted');
        return true;
      } catch (error) {
        console.error('Microphone permission error:', error);
        micPermissionGranted = false;
        
        if (error.name === 'NotAllowedError' || error.name === 'PermissionDeniedError') {
          showPermissionDeniedUI();
        } else if (error.name === 'NotFoundError') {
          showNoMicrophoneUI();
        } else {
          showGenericMicrophoneError(error);
        }
        return false;
      }
    }

    function showPermissionDeniedUI() {
      PERMISSION_ALERT.classList.remove('hidden');
      STATUS_TEXT.textContent = 'Microphone permission denied';
      STATUS_ICON.className = 'lucide lucide-mic-off';
      MIC_BTN.disabled = true;
      MIC_BTN.title = 'Microphone permission required';
    }

    function showNoMicrophoneUI() {
      STATUS_TEXT.textContent = 'No microphone found';
      STATUS_ICON.className = 'lucide lucide-mic-off';
      MIC_BTN.disabled = true;
      MIC_BTN.title = 'No microphone available';
    }

    function showGenericMicrophoneError(error) {
      STATUS_TEXT.textContent = 'Microphone access error';
      STATUS_ICON.className = 'lucide lucide-alert-circle';
      MIC_BTN.disabled = true;
      MIC_BTN.title = `Error: ${error.message}`;
      console.error('Generic microphone error:', error);
    }

    RETRY_PERMISSIONS.addEventListener('click', async () => {
      STATUS_TEXT.textContent = 'Requesting microphone access...';
      const granted = await checkMicrophonePermission();
      if (granted) {
        await initializeApp();
      }
    });

    // Audio visualizer
    async function startAudioVisualizer() {
      if (!micPermissionGranted) {
        console.warn('Cannot start visualizer - no microphone permission');
        return;
      }

      try {
        audioStream = await navigator.mediaDevices.getUserMedia({ 
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true
          }
        });
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        
        if (audioContext.state === 'suspended') {
          await audioContext.resume();
        }
        
        const source = audioContext.createMediaStreamSource(audioStream);
        analyser = audioContext.createAnalyser();
        analyser.fftSize = 256;
        analyser.smoothingTimeConstant = 0.8;
        source.connect(analyser);
        drawVisualizer();
        console.log('Audio visualizer started');
      } catch (e) {
        console.error('Audio visualizer error:', e);
        WAVE_ELEMENTS.forEach(w => w.classList.add('animated'));
      }
    }

    function stopAudioVisualizer() {
      if (rafId) {
        cancelAnimationFrame(rafId);
        rafId = null;
      }
      if (analyser) {
        analyser.disconnect();
        analyser = null;
      }
      if (audioContext && audioContext.state !== 'closed') {
        try { audioContext.close(); } catch (e) { console.warn('Error closing audio context:', e); }
        audioContext = null;
      }
      if (audioStream) {
        audioStream.getTracks().forEach(t => t.stop());
        audioStream = null;
      }
      WAVE_ELEMENTS.forEach(w => {
        w.style.height = '14px';
        w.classList.remove('animated');
        w.style.background = 'rgba(34,197,94,0.95)';
      });
      console.log('Audio visualizer stopped');
    }

    function drawVisualizer() {
      if (!analyser) return;
      const data = new Uint8Array(analyser.frequencyBinCount);
      analyser.getByteFrequencyData(data);

      const n = WAVE_ELEMENTS.length;
      for (let i = 0; i < n; i++) {
        const start = Math.floor(i * data.length / n);
        const end = Math.floor((i + 1) * data.length / n);
        let sum = 0;
        for (let j = start; j < end; j++) { sum += data[j]; }
        const avg = sum / (end - start || 1);
        const minH = 14, maxH = 86;
        const h = Math.max(minH, Math.min(maxH, Math.round((avg / 255) * (maxH - minH) + minH)));
        const bar = WAVE_ELEMENTS[i];
        bar.style.height = h + 'px';
        bar.classList.add('animated');
        const pct = avg / 255;
        if (pct > 0.6) bar.style.background = 'rgba(239,68,68,0.95)';
        else if (pct > 0.3) bar.style.background = 'rgba(34,197,94,0.95)';
        else bar.style.background = 'rgba(34,197,94,0.6)';
      }
      rafId = requestAnimationFrame(drawVisualizer);
    }

    function setIdleVisualizer() {
      WAVE_ELEMENTS.forEach((w, i) => {
        w.style.height = '14px';
        w.classList.remove('animated');
        w.style.background = 'rgba(34,197,94,0.95)';
      });
    }

    // Speech recognition
    function initSpeechRecognition() {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      if (!SpeechRecognition) {
        speechRecognitionSupported = false;
        return null;
      }
      
      speechRecognitionSupported = true;
      const r = new SpeechRecognition();
      r.lang = LANGUAGES[currentLanguage].code;
      r.interimResults = true;
      r.continuous = true;
      r.maxAlternatives = 1;
      
      return r;
    }

    function startListening() {
      if (!micPermissionGranted) {
        console.warn('Cannot start listening - no microphone permission');
        return;
      }

      if (!recognition) {
        recognition = initSpeechRecognition();
        if (!recognition) {
          alert("Speech Recognition API not supported by this browser. Use Chrome, Edge, or Safari.");
          return;
        }
        attachRecognitionHandlers();
      }

      try {
        console.log('Starting speech recognition...');
        recognition.lang = LANGUAGES[currentLanguage].code; // Update language
        recognition.start();
      } catch (e) {
        console.error('Failed to start recognition:', e);
        isListening = false;
        STATUS_TEXT.textContent = 'Failed to start listening';
        STATUS_ICON.className = 'lucide lucide-alert-circle';
      }
    }

    function stopListening() {
      if (recognition) {
        try { 
          console.log('Stopping speech recognition...');
          recognition.stop(); 
        } catch (e) {
          console.warn('Error stopping recognition:', e);
        }
      }
    }

    function attachRecognitionHandlers() {
      if (!recognition) return;

      recognition.onstart = () => {
        console.log('Speech recognition started');
        isListening = true;
        STATUS_TEXT.textContent = `Listening in ${LANGUAGES[currentLanguage].name}...`;
        STATUS_ICON.className = 'lucide lucide-mic';
        MIC_BTN.classList.remove('bg-emerald-500', 'mic-idle');
        MIC_BTN.classList.add('bg-red-600', 'mic-listening');
        MIC_ICON.className = 'lucide lucide-square text-3xl';
        startAudioVisualizer();
      };

      recognition.onend = () => {
        console.log('Speech recognition ended');
        isListening = false;
        STATUS_TEXT.textContent = 'Ready to assist you';
        STATUS_ICON.className = 'lucide lucide-mic-off';
        MIC_BTN.classList.remove('bg-red-600', 'mic-listening');
        MIC_BTN.classList.add('bg-emerald-500', 'mic-idle');
        MIC_ICON.className = 'lucide lucide-mic text-3xl';
        stopAudioVisualizer();
        
        if (interimTranscript && interimTranscript.trim()) {
          // Process the final transcript
          processUserInput(interimTranscript.trim());
          interimTranscript = '';
        }
        TRANSCRIPT.textContent = '';
      };

      recognition.onerror = (ev) => {
        console.error('SpeechRecognition error:', ev.error, ev);
        
        switch (ev.error) {
          case 'not-allowed':
            STATUS_TEXT.textContent = 'Microphone access denied';
            showPermissionDeniedUI();
            break;
          case 'no-speech':
            STATUS_TEXT.textContent = 'No speech detected - try again';
            break;
          case 'audio-capture':
            STATUS_TEXT.textContent = 'Audio capture failed';
            break;
          case 'network':
            STATUS_TEXT.textContent = 'Network error - check connection';
            break;
          default:
            STATUS_TEXT.textContent = `Speech error: ${ev.error}`;
        }
        
        isListening = false;
      };

      recognition.onresult = (event) => {
        let combined = '';
        let finalTranscript = '';
        
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const res = event.results[i];
          const transcript = res[0].transcript;
          
          if (res.isFinal) {
            finalTranscript += transcript;
            console.log('Final transcript:', transcript);
          } else {
            combined += transcript;
          }
        }
        
        interimTranscript = finalTranscript || combined;
        TRANSCRIPT.textContent = interimTranscript;
      };
    }

    // Mic button handler
    MIC_BTN.addEventListener('click', async () => {
      if (!micPermissionGranted) {
        const granted = await checkMicrophonePermission();
        if (!granted) {
          console.warn('Microphone permission still not granted');
          return;
        }
      }

      if (isListening) {
        stopListening();
      } else {
        if (!recognition) {
          recognition = initSpeechRecognition();
          if (!recognition) {
            alert("Speech Recognition API not supported by this browser. Use Chrome, Edge, or Safari.");
            return;
          }
          attachRecognitionHandlers();
        }
        startListening();
      }
    });

    // Network status
    function updateOnlineStatus() {
      const online = navigator.onLine;
      ONLINE_TEXT.textContent = online ? 'Online' : 'Offline';
      WIFI_ICON.className = online ? 'lucide lucide-wifi' : 'lucide lucide-wifi-off';
      ONLINE_TEXT.style.color = online ? '' : '#ef4444';
    }
    window.addEventListener('online', updateOnlineStatus);
    window.addEventListener('offline', updateOnlineStatus);

    // Initialize the app
    async function initializeApp() {
      loadConfig();
      loadHistory();
      renderHistory();
      setIdleVisualizer();
      updateOnlineStatus();

      HISTORY_PANEL.style.maxHeight = '12rem';
      BTN_TOGGLE_HISTORY.textContent = 'Collapse';

      if (!(window.SpeechRecognition || window.webkitSpeechRecognition)) {
        STATUS_TEXT.textContent = "Speech Recognition not supported in this browser.";
        STATUS_ICON.className = 'lucide lucide-alert-circle';
        MIC_BTN.disabled = true;
        return;
      }

      console.log('Checking microphone permissions...');
      const permissionGranted = await checkMicrophonePermission();
      
      if (permissionGranted) {
        STATUS_TEXT.textContent = 'Ready to assist you';
        STATUS_ICON.className = 'lucide lucide-mic-off';
        MIC_BTN.disabled = false;
        MIC_BTN.title = 'Tap to start/stop listening';
      }

      // Check if API key is configured
      if (!GEMINI_API_KEY) {
        STATUS_TEXT.textContent = 'Please configure API key in the code';
        setTimeout(() => {
          STATUS_TEXT.textContent = 'Ready to assist you';
        }, 3000);
      }
    }

    // Initialize
    (function init() {
      initializeApp();
    })();

    // Development helpers
    TRANSCRIPT.addEventListener('dblclick', () => {
      const text = prompt('Add manual transcript to history (dev helper):', '');
      if (text) { 
        processUserInput(text);
      }
    });

    // Expose API for testing
    window.FarmAssist = { 
      addHistoryEntry,
      processUserInput,
      speakText,
      checkMicrophonePermission,
      micPermissionGranted: () => micPermissionGranted,
      speechRecognitionSupported: () => speechRecognitionSupported
    };

    console.log('FarmAssist Multilingual Voice Assistant initialized');
    console.log('Supported languages:', Object.keys(LANGUAGES));
    console.log('Browser support check:', {
      getUserMedia: !!navigator.mediaDevices?.getUserMedia,
      speechRecognition: !!(window.SpeechRecognition || window.webkitSpeechRecognition),
      speechSynthesis: !!window.speechSynthesis,
      permissions: !!navigator.permissions,
      audioContext: !!(window.AudioContext || window.webkitAudioContext)
    });
    console.log('API Key configured:', !!GEMINI_API_KEY);

  </script>
</body>
</html>
